{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preparation - Proyecto integrador"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El análisis de texto es un campo creciente. Las ingentes cantidades de texto, representan una oportunidad para las organizaciones que desean encontrar patrones, conocer las opiniones de sus consumidores, analizar posibles coocurrencias, e inclusive, generar predicciones sobre las palabras que el usuario digitará, o traducir en tiempo real un texto entre distintos idiomas.\n",
    "\n",
    "En el notebook se busca utilizar las funciones disponibles para obtener información líquida suceptible de analizar basados en el cálculo de frecuencua de palabras, aplicación de term frencuency e idf y posteriormente el Bag Of Words para la base de datos que fue extraida de Twitter para la cuenta de avianca.\n",
    "\n",
    "Diccionario a utilizar: download es_core_news_sm\n",
    "Para más información sobre spacy: https://spacy.io/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los estudiantes que participaron en el desarrollo de este proyecto son:\n",
    "    \n",
    "    1. Jorge Luis Renteria\n",
    "    2. Edgar Leandro Jimenez\n",
    "    3. Jesus Alberto Arcia\n",
    "    \n",
    "Universidad Eafit, 2019."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importacion de librerias necesarias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs\n",
    "import spacy\n",
    "from tokenize import tokenize, untokenize, NUMBER, STRING, NAME, OP\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from nltk.collocations import *\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import matplotlib.pyplot as plt\n",
    "import heapq\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "#nltk.download('stopwords')\n",
    "nlp=spacy.load('es_core_news_sm')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Carga de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel('TweetsEntrenamiento.xlsx')\n",
    "\n",
    "# Extraemos el texto\n",
    "corpus= list(df['Texto'])\n",
    "MiListaStops=list(set(stopwords.words('spanish')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Limpieza de todos los tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,tweet in enumerate(corpus):\n",
    "    tweet=re.sub('@([\\w.]+ )','',re.sub('https.*','',tweet))\n",
    "    tweet=re.sub('#[\\w]*','',tweet)\n",
    "    tweet = tweet.lower()\n",
    "    tweet = re.sub(r'\\W',' ',re.sub(r'\\s+',' ',tweet))\n",
    "    tweet = re.sub('¡','',re.sub('°','',tweet))\n",
    "    tweet=tweet.translate(str.maketrans('', '', string.punctuation))\n",
    "    for word in tweet.split():\n",
    "        if str(word.lower()) in MiListaStops:\n",
    "            tweet=re.sub(r'\\b'+str(word)+r'\\b','',tweet)\n",
    "    tweet=re.sub(' +',' ',tweet)\n",
    "    corpus[i] = tweet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Realizamos el conteo de las palabras "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokens: \n",
    "\n",
    "Una base de datos de texto (o corpus) es una agrupación de bytes. El texto en su forma más pura, es una colección de bytes (o caracteres). La mayoria de veces es útil agrupar estos caracteres en unidades continuas llamadas tokens. En español, al igual que en la mayoria de los idiomas occidentales, un token corresponde a palabras y sequencias numericas separadas por espacios en blanco o signos de puntuación. El proceso de reducir un texto a tokens se conoce como tokenización."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Lemmas\n",
    "Lemmas: Lemas corresponde a la raíz de una palabra. Considere por ejemplo la palabra correr. Esta puede tener distintas formas como corriendo, corrí, correré, entre otras. En este caso, resulta útil reducir la palabra a su raíz o lemma. En este caso, este proceso se conoce como lematization. Obtener los lemas es bastante sencillo una vez se ha procesado el texto con la funcion nlp de Spacy, y se logra a través de la función .lemma_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordfreq = {}\n",
    "for sentence in corpus:\n",
    "    tokens = nltk.word_tokenize(sentence)\n",
    "    tokens=nlp(sentence)\n",
    "    for token in tokens:\n",
    "        if not str(token) in list([' ']):    \n",
    "            token=token.lemma_\n",
    "            if token not in wordfreq.keys():\n",
    "                wordfreq[token] = 1\n",
    "            else:\n",
    "                wordfreq[token] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exraemos las palabras mas frecuentes (Para este caso utilizamos todas)\n",
    "most_freq = heapq.nlargest(4786, wordfreq, key=wordfreq.get)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creamos una lista con todos los tweets y su tokenizacion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "milista=[]\n",
    "for document in corpus:\n",
    "    milista.append(list(str(nlp(document)).split()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Proceso para calcular los tf-idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_idf_values = {}\n",
    "\n",
    "for token in most_freq:\n",
    "    doc_containing_word = 0\n",
    "    for document in milista:\n",
    "        if str(token) in document:\n",
    "            doc_containing_word += 1\n",
    "    word_idf_values[token] = np.log(len(corpus)/(1 + doc_containing_word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_tf_values = {}\n",
    "\n",
    "for token in most_freq:\n",
    "    sent_tf_vector = []\n",
    "    for document in milista:\n",
    "        doc_freq = document.count(token)\n",
    "        if len(document)==0:\n",
    "            continue\n",
    "        word_tf = doc_freq/len(document)\n",
    "        sent_tf_vector.append(word_tf)\n",
    "    word_tf_values[token] = sent_tf_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_values = []\n",
    "\n",
    "for token in word_tf_values.keys():\n",
    "    tfidf_sentences = []\n",
    "    for tf_sentence in word_tf_values[token]:\n",
    "        tf_idf_score = tf_sentence * word_idf_values[token]\n",
    "        tfidf_sentences.append(tf_idf_score)\n",
    "    tfidf_values.append(tfidf_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_model = np.asarray(tfidf_values)\n",
    "tf_idf_model = np.transpose(tf_idf_model)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       ...,\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.42967741, 0.42967741,\n",
       "        0.42967741]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_idf_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(tf_idf_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convertimos a data frame el resultado final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = pd.DataFrame(tf_idf_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ahora asociamos el sentimiento a cada fila (Tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final['label']=df['Calificación sentimiento']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exportamos a csv para guardar en nube"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final.to_csv(\"bow\" + \".csv\", encoding='utf')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
