{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelado para predicción de sentimientos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esta modelación parte a partir de los datos que han sido extraidos vía Twitter y han sido limpiados a traves del cleaning presentado en la 02-Limpieza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lectura de librerias\n",
    "\n",
    "library(twitteR)\n",
    "library(twitteR)\n",
    "library(ROAuth)\n",
    "library(plyr)\n",
    "library(dplyr)\n",
    "library(stringr)\n",
    "library(ggplot2)\n",
    "library(devtools)\n",
    "library(tidyverse)\n",
    "library(text2vec)\n",
    "library(caret)\n",
    "library(glmnet)\n",
    "library(ggrepel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Cargando base de datos de tweets para entrenamiento\n",
    "\n",
    "tweets_classified <- read_csv('dataset_training.csv',\n",
    "                              col_names = c('sentiment', 'id', 'date', 'query', 'user', 'text')) %>%\n",
    "  # converting some symbols\n",
    "  dmap_at('text', conv_fun) %>%\n",
    "  # replacing class values\n",
    "  mutate(sentiment = ifelse(sentiment == 0, 0, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Limpieza de NA\n",
    "\n",
    "tweets_classified_na <- tweets_classified %>%\n",
    " filter(is.na(id) == TRUE) %>%\n",
    " mutate(id = c(1:n()))\n",
    "\n",
    "\n",
    "tweets_classified <- tweets_classified %>%\n",
    " filter(!is.na(id)) %>%\n",
    " rbind(., tweets_classified_na)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Partición de dato\n",
    "\n",
    "# Inicialización semilla para resultados reproducibles\n",
    "set.seed(2340)\n",
    "\n",
    "\n",
    "trainIndex <- createDataPartition(tweets_classified$sentiment, p = 0.8, \n",
    "                                  list = F, \n",
    "                                  times = 1)\n",
    "\n",
    "\n",
    "# Partición del dataser\n",
    "tweets_train <- tweets_classified[trainIndex, ]\n",
    "tweets_test <- tweets_classified[-trainIndex, ]\n",
    "\n",
    "\n",
    "# Tokenización de los tweets\n",
    "prep_fun <- tolower\n",
    "tok_fun <- word_tokenizer\n",
    "\n",
    "it_train <- itoken(tweets_train$text, \n",
    "                   preprocessor = prep_fun, \n",
    "                   tokenizer = tok_fun,\n",
    "                   ids = tweets_train$id,\n",
    "                   progressbar = TRUE)\n",
    "\n",
    "it_test <- itoken(tweets_test$text, \n",
    "                  preprocessor = prep_fun, \n",
    "                  tokenizer = tok_fun,\n",
    "                  ids = tweets_test$id,\n",
    "                  progressbar = TRUE)\n",
    "\n",
    "# Vocacbulario\n",
    "vocab            <- create_vocabulary(it_train)\n",
    "vectorizer       <- vocab_vectorizer(vocab)\n",
    "dtm_train        <- create_dtm(it_train, vectorizer)\n",
    "dtm_test         <- create_dtm(it_test, vectorizer)\n",
    "\n",
    "tfidf            <- TfIdf$new()\n",
    "\n",
    "# Ajuste modelo y transformación\n",
    "dtm_train_tfidf  <- fit_transform(dtm_train, tfidf)\n",
    "dtm_test_tfidf   <- fit_transform(dtm_test, tfidf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Entrenamiento del modelo\n",
    "t1 <- Sys.time()\n",
    "\n",
    "glmnet_classifier <- cv.glmnet(x = dtm_train_tfidf, \n",
    "                               y = tweets_train[['sentiment']], \n",
    "                               family = 'binomial', \n",
    "                               alpha = 1,\n",
    "                               type.measure = \"auc\", # Area bajo la curca ROC\n",
    "                               nfolds = 5,\n",
    "                               thresh = 1e-3,\n",
    "                               maxit = 1e3)\n",
    "t2 <- Sys.time()\n",
    "t2-t1 # Tiempo de ejecución del entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graficas de modelación\n",
    "options(repr.plot.width=10, repr.plot.height=4)\n",
    "print(difftime(Sys.time(), t1, units = 'mins'))\n",
    "\n",
    "plot(glmnet_classifier)\n",
    "print(paste(\"max AUC =\", round(max(glmnet_classifier$cvm), 4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicciones del modelo\n",
    "preds <- predict(glmnet_classifier,\n",
    "                 dtm_test_tfidf, \n",
    "                 type = 'response')[ ,1]\n",
    "\n",
    "glmnet:::auc(as.numeric(tweets_test$sentiment), preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardar los resultados del modelo ajustado\n",
    "saveRDS(glmnet_classifier, 'glmnet_classifier.RDS')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graficando nube de palabras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Generación de nube de datos - Wordcloud\n",
    "library(wordcloud)\n",
    "\n",
    "corpus       <- Corpus(VectorSource(corpus))\n",
    "corpus       <- tm_map(corpus, toSpace, x)\n",
    "\n",
    "#convierte el corpus en un documento-elemento matriz\n",
    "tdm         <- TermDocumentMatrix(corpus,\n",
    "                                  control = list(removePunctuation = TRUE,\n",
    "                                                 stopwords = c(stopwords(\"spanish\"), stopwords(\"es\"), x),\n",
    "                                                 removeNumbers = TRUE, tolower = TRUE))\n",
    "\n",
    "# Convierte el documento en un formato matrix\n",
    "m = as.matrix(tdm)\n",
    "\n",
    "# get word counts in decreasing order\n",
    "word_freqs <- sort(rowSums(m), decreasing=TRUE)\n",
    "\n",
    "# create a data frame with words and their frequencies\n",
    "dm         <- data.frame(word=names(word_freqs), freq=word_freqs)\n",
    "\n",
    "# plot wordcloud\n",
    "options(repr.plot.width=7, repr.plot.height=7)\n",
    "\n",
    "wordcloud(dm$word, dm$freq, random.order=FALSE, colors=brewer.pal(8, \"Dark2\"))\n",
    "lapply(data, function(x) write.table( data.frame(x), 'test.csv'  , append= T, sep=',' ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fuente"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esta modelación y código se baso en la siguiente publicación:\n",
    "\n",
    "https://www.r-bloggers.com/twitter-sentiment-analysis-with-machine-learning-in-r-using-doc2vec-approach/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
