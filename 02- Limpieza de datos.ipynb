{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Limpieza de texto en Python - Proyecto integrador\n",
    "\n",
    "\n",
    "## Introducción:\n",
    "\n",
    "El análisis de texto e imagenes es un campo creciente. Las ingentes cantidades de texto, imagenes y video representan una oportunidad para las organizaciones que desean encontrar patrones, conocer las opiniones de sus consumidores, analizar posibles coocurrencias, e inclusive, generar predicciones sobre las palabras que el usuario digitará, o traducir en tiempo real un texto entre distintos idiomas.\n",
    "\n",
    "En el notebook se busca utilizar las funciones disponibles para obtener información líquida suceptible de analizar basados en el cálculo de frecuencua de palabras, concurrencia entre frases y datos gráficos exploratorios.\n",
    "\n",
    "\n",
    "Diccionario a utilizar: download es_core_news_sm\n",
    "-\n",
    "\n",
    "Para más información sobre spacy: https://spacy.io/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Panorama genral de la Analitica de Texto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Introducción"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs\n",
    "import spacy\n",
    "\n",
    "nlp=spacy.load('es_core_news_md')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Input del tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Información que se va a cargar de cada uno de los tweets\n"
     ]
    }
   ],
   "source": [
    "text = 'Información que se va a cargar de cada uno de los tweets'\n",
    "print(text[0:10*100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### El Asunto del Orden del texto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El orden de las palabras es importante. Las reglas que rigen el orden de las palabras en una secuencia de palabras (como una oración) se llaman gramática de un idioma. Si solo desea codificar el sentido general y el sentimiento de una oración corta, el orden de las palabras no es muy importante"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Información que se',\n",
       " 'Información que va',\n",
       " 'Información que a',\n",
       " 'Información que cargar',\n",
       " 'Información que de',\n",
       " 'Información que cada',\n",
       " 'Información que uno',\n",
       " 'Información que de',\n",
       " 'Información que los',\n",
       " 'Información que tweets',\n",
       " 'Información se que',\n",
       " 'Información se va',\n",
       " 'Información se a',\n",
       " 'Información se cargar',\n",
       " 'Información se de',\n",
       " 'Información se cada',\n",
       " 'Información se uno',\n",
       " 'Información se de',\n",
       " 'Información se los',\n",
       " 'Información se tweets',\n",
       " 'Información va que',\n",
       " 'Información va se',\n",
       " 'Información va a',\n",
       " 'Información va cargar',\n",
       " 'Información va de',\n",
       " 'Información va cada',\n",
       " 'Información va uno',\n",
       " 'Información va de',\n",
       " 'Información va los',\n",
       " 'Información va tweets',\n",
       " 'Información a que',\n",
       " 'Información a se',\n",
       " 'Información a va',\n",
       " 'Información a cargar',\n",
       " 'Información a de',\n",
       " 'Información a cada',\n",
       " 'Información a uno',\n",
       " 'Información a de',\n",
       " 'Información a los',\n",
       " 'Información a tweets',\n",
       " 'Información cargar que',\n",
       " 'Información cargar se',\n",
       " 'Información cargar va',\n",
       " 'Información cargar a',\n",
       " 'Información cargar de',\n",
       " 'Información cargar cada',\n",
       " 'Información cargar uno',\n",
       " 'Información cargar de',\n",
       " 'Información cargar los',\n",
       " 'Información cargar tweets',\n",
       " 'Información de que',\n",
       " 'Información de se',\n",
       " 'Información de va',\n",
       " 'Información de a',\n",
       " 'Información de cargar',\n",
       " 'Información de cada',\n",
       " 'Información de uno',\n",
       " 'Información de de',\n",
       " 'Información de los',\n",
       " 'Información de tweets',\n",
       " 'Información cada que',\n",
       " 'Información cada se',\n",
       " 'Información cada va',\n",
       " 'Información cada a',\n",
       " 'Información cada cargar',\n",
       " 'Información cada de',\n",
       " 'Información cada uno',\n",
       " 'Información cada de',\n",
       " 'Información cada los',\n",
       " 'Información cada tweets',\n",
       " 'Información uno que',\n",
       " 'Información uno se',\n",
       " 'Información uno va',\n",
       " 'Información uno a',\n",
       " 'Información uno cargar',\n",
       " 'Información uno de',\n",
       " 'Información uno cada',\n",
       " 'Información uno de',\n",
       " 'Información uno los',\n",
       " 'Información uno tweets',\n",
       " 'Información de que',\n",
       " 'Información de se',\n",
       " 'Información de va',\n",
       " 'Información de a',\n",
       " 'Información de cargar',\n",
       " 'Información de de',\n",
       " 'Información de cada',\n",
       " 'Información de uno',\n",
       " 'Información de los',\n",
       " 'Información de tweets',\n",
       " 'Información los que',\n",
       " 'Información los se',\n",
       " 'Información los va',\n",
       " 'Información los a',\n",
       " 'Información los cargar',\n",
       " 'Información los de',\n",
       " 'Información los cada',\n",
       " 'Información los uno',\n",
       " 'Información los de',\n",
       " 'Información los tweets',\n",
       " 'Información tweets que',\n",
       " 'Información tweets se',\n",
       " 'Información tweets va',\n",
       " 'Información tweets a',\n",
       " 'Información tweets cargar',\n",
       " 'Información tweets de',\n",
       " 'Información tweets cada',\n",
       " 'Información tweets uno',\n",
       " 'Información tweets de',\n",
       " 'Información tweets los',\n",
       " 'que Información se',\n",
       " 'que Información va',\n",
       " 'que Información a',\n",
       " 'que Información cargar',\n",
       " 'que Información de',\n",
       " 'que Información cada',\n",
       " 'que Información uno',\n",
       " 'que Información de',\n",
       " 'que Información los',\n",
       " 'que Información tweets',\n",
       " 'que se Información',\n",
       " 'que se va',\n",
       " 'que se a',\n",
       " 'que se cargar',\n",
       " 'que se de',\n",
       " 'que se cada',\n",
       " 'que se uno',\n",
       " 'que se de',\n",
       " 'que se los',\n",
       " 'que se tweets',\n",
       " 'que va Información',\n",
       " 'que va se',\n",
       " 'que va a',\n",
       " 'que va cargar',\n",
       " 'que va de',\n",
       " 'que va cada',\n",
       " 'que va uno',\n",
       " 'que va de',\n",
       " 'que va los',\n",
       " 'que va tweets',\n",
       " 'que a Información',\n",
       " 'que a se',\n",
       " 'que a va',\n",
       " 'que a cargar',\n",
       " 'que a de',\n",
       " 'que a cada',\n",
       " 'que a uno',\n",
       " 'que a de',\n",
       " 'que a los',\n",
       " 'que a tweets',\n",
       " 'que cargar Información',\n",
       " 'que cargar se',\n",
       " 'que cargar va',\n",
       " 'que cargar a',\n",
       " 'que cargar de',\n",
       " 'que cargar cada',\n",
       " 'que cargar uno',\n",
       " 'que cargar de',\n",
       " 'que cargar los',\n",
       " 'que cargar tweets',\n",
       " 'que de Información',\n",
       " 'que de se',\n",
       " 'que de va',\n",
       " 'que de a',\n",
       " 'que de cargar',\n",
       " 'que de cada',\n",
       " 'que de uno',\n",
       " 'que de de',\n",
       " 'que de los',\n",
       " 'que de tweets',\n",
       " 'que cada Información',\n",
       " 'que cada se',\n",
       " 'que cada va',\n",
       " 'que cada a',\n",
       " 'que cada cargar',\n",
       " 'que cada de',\n",
       " 'que cada uno',\n",
       " 'que cada de',\n",
       " 'que cada los',\n",
       " 'que cada tweets',\n",
       " 'que uno Información',\n",
       " 'que uno se',\n",
       " 'que uno va',\n",
       " 'que uno a',\n",
       " 'que uno cargar',\n",
       " 'que uno de',\n",
       " 'que uno cada',\n",
       " 'que uno de',\n",
       " 'que uno los',\n",
       " 'que uno tweets',\n",
       " 'que de Información',\n",
       " 'que de se',\n",
       " 'que de va',\n",
       " 'que de a',\n",
       " 'que de cargar',\n",
       " 'que de de',\n",
       " 'que de cada',\n",
       " 'que de uno',\n",
       " 'que de los',\n",
       " 'que de tweets',\n",
       " 'que los Información',\n",
       " 'que los se',\n",
       " 'que los va',\n",
       " 'que los a',\n",
       " 'que los cargar',\n",
       " 'que los de',\n",
       " 'que los cada',\n",
       " 'que los uno',\n",
       " 'que los de',\n",
       " 'que los tweets',\n",
       " 'que tweets Información',\n",
       " 'que tweets se',\n",
       " 'que tweets va',\n",
       " 'que tweets a',\n",
       " 'que tweets cargar',\n",
       " 'que tweets de',\n",
       " 'que tweets cada',\n",
       " 'que tweets uno',\n",
       " 'que tweets de',\n",
       " 'que tweets los',\n",
       " 'se Información que',\n",
       " 'se Información va',\n",
       " 'se Información a',\n",
       " 'se Información cargar',\n",
       " 'se Información de',\n",
       " 'se Información cada',\n",
       " 'se Información uno',\n",
       " 'se Información de',\n",
       " 'se Información los',\n",
       " 'se Información tweets',\n",
       " 'se que Información',\n",
       " 'se que va',\n",
       " 'se que a',\n",
       " 'se que cargar',\n",
       " 'se que de',\n",
       " 'se que cada',\n",
       " 'se que uno',\n",
       " 'se que de',\n",
       " 'se que los',\n",
       " 'se que tweets',\n",
       " 'se va Información',\n",
       " 'se va que',\n",
       " 'se va a',\n",
       " 'se va cargar',\n",
       " 'se va de',\n",
       " 'se va cada',\n",
       " 'se va uno',\n",
       " 'se va de',\n",
       " 'se va los',\n",
       " 'se va tweets',\n",
       " 'se a Información',\n",
       " 'se a que',\n",
       " 'se a va',\n",
       " 'se a cargar',\n",
       " 'se a de',\n",
       " 'se a cada',\n",
       " 'se a uno',\n",
       " 'se a de',\n",
       " 'se a los',\n",
       " 'se a tweets',\n",
       " 'se cargar Información',\n",
       " 'se cargar que',\n",
       " 'se cargar va',\n",
       " 'se cargar a',\n",
       " 'se cargar de',\n",
       " 'se cargar cada',\n",
       " 'se cargar uno',\n",
       " 'se cargar de',\n",
       " 'se cargar los',\n",
       " 'se cargar tweets',\n",
       " 'se de Información',\n",
       " 'se de que',\n",
       " 'se de va',\n",
       " 'se de a',\n",
       " 'se de cargar',\n",
       " 'se de cada',\n",
       " 'se de uno',\n",
       " 'se de de',\n",
       " 'se de los',\n",
       " 'se de tweets',\n",
       " 'se cada Información',\n",
       " 'se cada que',\n",
       " 'se cada va',\n",
       " 'se cada a',\n",
       " 'se cada cargar',\n",
       " 'se cada de',\n",
       " 'se cada uno',\n",
       " 'se cada de',\n",
       " 'se cada los',\n",
       " 'se cada tweets',\n",
       " 'se uno Información',\n",
       " 'se uno que',\n",
       " 'se uno va',\n",
       " 'se uno a',\n",
       " 'se uno cargar',\n",
       " 'se uno de',\n",
       " 'se uno cada',\n",
       " 'se uno de',\n",
       " 'se uno los',\n",
       " 'se uno tweets',\n",
       " 'se de Información',\n",
       " 'se de que',\n",
       " 'se de va',\n",
       " 'se de a',\n",
       " 'se de cargar',\n",
       " 'se de de',\n",
       " 'se de cada',\n",
       " 'se de uno',\n",
       " 'se de los',\n",
       " 'se de tweets',\n",
       " 'se los Información',\n",
       " 'se los que',\n",
       " 'se los va',\n",
       " 'se los a',\n",
       " 'se los cargar',\n",
       " 'se los de',\n",
       " 'se los cada',\n",
       " 'se los uno',\n",
       " 'se los de',\n",
       " 'se los tweets',\n",
       " 'se tweets Información',\n",
       " 'se tweets que',\n",
       " 'se tweets va',\n",
       " 'se tweets a',\n",
       " 'se tweets cargar',\n",
       " 'se tweets de',\n",
       " 'se tweets cada',\n",
       " 'se tweets uno',\n",
       " 'se tweets de',\n",
       " 'se tweets los',\n",
       " 'va Información que',\n",
       " 'va Información se',\n",
       " 'va Información a',\n",
       " 'va Información cargar',\n",
       " 'va Información de',\n",
       " 'va Información cada',\n",
       " 'va Información uno',\n",
       " 'va Información de',\n",
       " 'va Información los',\n",
       " 'va Información tweets',\n",
       " 'va que Información',\n",
       " 'va que se',\n",
       " 'va que a',\n",
       " 'va que cargar',\n",
       " 'va que de',\n",
       " 'va que cada',\n",
       " 'va que uno',\n",
       " 'va que de',\n",
       " 'va que los',\n",
       " 'va que tweets',\n",
       " 'va se Información',\n",
       " 'va se que',\n",
       " 'va se a',\n",
       " 'va se cargar',\n",
       " 'va se de',\n",
       " 'va se cada',\n",
       " 'va se uno',\n",
       " 'va se de',\n",
       " 'va se los',\n",
       " 'va se tweets',\n",
       " 'va a Información',\n",
       " 'va a que',\n",
       " 'va a se',\n",
       " 'va a cargar',\n",
       " 'va a de',\n",
       " 'va a cada',\n",
       " 'va a uno',\n",
       " 'va a de',\n",
       " 'va a los',\n",
       " 'va a tweets',\n",
       " 'va cargar Información',\n",
       " 'va cargar que',\n",
       " 'va cargar se',\n",
       " 'va cargar a',\n",
       " 'va cargar de',\n",
       " 'va cargar cada',\n",
       " 'va cargar uno',\n",
       " 'va cargar de',\n",
       " 'va cargar los',\n",
       " 'va cargar tweets',\n",
       " 'va de Información',\n",
       " 'va de que',\n",
       " 'va de se',\n",
       " 'va de a',\n",
       " 'va de cargar',\n",
       " 'va de cada',\n",
       " 'va de uno',\n",
       " 'va de de',\n",
       " 'va de los',\n",
       " 'va de tweets',\n",
       " 'va cada Información',\n",
       " 'va cada que',\n",
       " 'va cada se',\n",
       " 'va cada a',\n",
       " 'va cada cargar',\n",
       " 'va cada de',\n",
       " 'va cada uno',\n",
       " 'va cada de',\n",
       " 'va cada los',\n",
       " 'va cada tweets',\n",
       " 'va uno Información',\n",
       " 'va uno que',\n",
       " 'va uno se',\n",
       " 'va uno a',\n",
       " 'va uno cargar',\n",
       " 'va uno de',\n",
       " 'va uno cada',\n",
       " 'va uno de',\n",
       " 'va uno los',\n",
       " 'va uno tweets',\n",
       " 'va de Información',\n",
       " 'va de que',\n",
       " 'va de se',\n",
       " 'va de a',\n",
       " 'va de cargar',\n",
       " 'va de de',\n",
       " 'va de cada',\n",
       " 'va de uno',\n",
       " 'va de los',\n",
       " 'va de tweets',\n",
       " 'va los Información',\n",
       " 'va los que',\n",
       " 'va los se',\n",
       " 'va los a',\n",
       " 'va los cargar',\n",
       " 'va los de',\n",
       " 'va los cada',\n",
       " 'va los uno',\n",
       " 'va los de',\n",
       " 'va los tweets',\n",
       " 'va tweets Información',\n",
       " 'va tweets que',\n",
       " 'va tweets se',\n",
       " 'va tweets a',\n",
       " 'va tweets cargar',\n",
       " 'va tweets de',\n",
       " 'va tweets cada',\n",
       " 'va tweets uno',\n",
       " 'va tweets de',\n",
       " 'va tweets los',\n",
       " 'a Información que',\n",
       " 'a Información se',\n",
       " 'a Información va',\n",
       " 'a Información cargar',\n",
       " 'a Información de',\n",
       " 'a Información cada',\n",
       " 'a Información uno',\n",
       " 'a Información de',\n",
       " 'a Información los',\n",
       " 'a Información tweets',\n",
       " 'a que Información',\n",
       " 'a que se',\n",
       " 'a que va',\n",
       " 'a que cargar',\n",
       " 'a que de',\n",
       " 'a que cada',\n",
       " 'a que uno',\n",
       " 'a que de',\n",
       " 'a que los',\n",
       " 'a que tweets',\n",
       " 'a se Información',\n",
       " 'a se que',\n",
       " 'a se va',\n",
       " 'a se cargar',\n",
       " 'a se de',\n",
       " 'a se cada',\n",
       " 'a se uno',\n",
       " 'a se de',\n",
       " 'a se los',\n",
       " 'a se tweets',\n",
       " 'a va Información',\n",
       " 'a va que',\n",
       " 'a va se',\n",
       " 'a va cargar',\n",
       " 'a va de',\n",
       " 'a va cada',\n",
       " 'a va uno',\n",
       " 'a va de',\n",
       " 'a va los',\n",
       " 'a va tweets',\n",
       " 'a cargar Información',\n",
       " 'a cargar que',\n",
       " 'a cargar se',\n",
       " 'a cargar va',\n",
       " 'a cargar de',\n",
       " 'a cargar cada',\n",
       " 'a cargar uno',\n",
       " 'a cargar de',\n",
       " 'a cargar los',\n",
       " 'a cargar tweets',\n",
       " 'a de Información',\n",
       " 'a de que',\n",
       " 'a de se',\n",
       " 'a de va',\n",
       " 'a de cargar',\n",
       " 'a de cada',\n",
       " 'a de uno',\n",
       " 'a de de',\n",
       " 'a de los',\n",
       " 'a de tweets',\n",
       " 'a cada Información',\n",
       " 'a cada que',\n",
       " 'a cada se',\n",
       " 'a cada va',\n",
       " 'a cada cargar',\n",
       " 'a cada de',\n",
       " 'a cada uno',\n",
       " 'a cada de',\n",
       " 'a cada los',\n",
       " 'a cada tweets',\n",
       " 'a uno Información',\n",
       " 'a uno que',\n",
       " 'a uno se',\n",
       " 'a uno va',\n",
       " 'a uno cargar',\n",
       " 'a uno de',\n",
       " 'a uno cada',\n",
       " 'a uno de',\n",
       " 'a uno los',\n",
       " 'a uno tweets',\n",
       " 'a de Información',\n",
       " 'a de que',\n",
       " 'a de se',\n",
       " 'a de va',\n",
       " 'a de cargar',\n",
       " 'a de de',\n",
       " 'a de cada',\n",
       " 'a de uno',\n",
       " 'a de los',\n",
       " 'a de tweets',\n",
       " 'a los Información',\n",
       " 'a los que',\n",
       " 'a los se',\n",
       " 'a los va',\n",
       " 'a los cargar',\n",
       " 'a los de',\n",
       " 'a los cada',\n",
       " 'a los uno',\n",
       " 'a los de',\n",
       " 'a los tweets',\n",
       " 'a tweets Información',\n",
       " 'a tweets que',\n",
       " 'a tweets se',\n",
       " 'a tweets va',\n",
       " 'a tweets cargar',\n",
       " 'a tweets de',\n",
       " 'a tweets cada',\n",
       " 'a tweets uno',\n",
       " 'a tweets de',\n",
       " 'a tweets los',\n",
       " 'cargar Información que',\n",
       " 'cargar Información se',\n",
       " 'cargar Información va',\n",
       " 'cargar Información a',\n",
       " 'cargar Información de',\n",
       " 'cargar Información cada',\n",
       " 'cargar Información uno',\n",
       " 'cargar Información de',\n",
       " 'cargar Información los',\n",
       " 'cargar Información tweets',\n",
       " 'cargar que Información',\n",
       " 'cargar que se',\n",
       " 'cargar que va',\n",
       " 'cargar que a',\n",
       " 'cargar que de',\n",
       " 'cargar que cada',\n",
       " 'cargar que uno',\n",
       " 'cargar que de',\n",
       " 'cargar que los',\n",
       " 'cargar que tweets',\n",
       " 'cargar se Información',\n",
       " 'cargar se que',\n",
       " 'cargar se va',\n",
       " 'cargar se a',\n",
       " 'cargar se de',\n",
       " 'cargar se cada',\n",
       " 'cargar se uno',\n",
       " 'cargar se de',\n",
       " 'cargar se los',\n",
       " 'cargar se tweets',\n",
       " 'cargar va Información',\n",
       " 'cargar va que',\n",
       " 'cargar va se',\n",
       " 'cargar va a',\n",
       " 'cargar va de',\n",
       " 'cargar va cada',\n",
       " 'cargar va uno',\n",
       " 'cargar va de',\n",
       " 'cargar va los',\n",
       " 'cargar va tweets',\n",
       " 'cargar a Información',\n",
       " 'cargar a que',\n",
       " 'cargar a se',\n",
       " 'cargar a va',\n",
       " 'cargar a de',\n",
       " 'cargar a cada',\n",
       " 'cargar a uno',\n",
       " 'cargar a de',\n",
       " 'cargar a los',\n",
       " 'cargar a tweets',\n",
       " 'cargar de Información',\n",
       " 'cargar de que',\n",
       " 'cargar de se',\n",
       " 'cargar de va',\n",
       " 'cargar de a',\n",
       " 'cargar de cada',\n",
       " 'cargar de uno',\n",
       " 'cargar de de',\n",
       " 'cargar de los',\n",
       " 'cargar de tweets',\n",
       " 'cargar cada Información',\n",
       " 'cargar cada que',\n",
       " 'cargar cada se',\n",
       " 'cargar cada va',\n",
       " 'cargar cada a',\n",
       " 'cargar cada de',\n",
       " 'cargar cada uno',\n",
       " 'cargar cada de',\n",
       " 'cargar cada los',\n",
       " 'cargar cada tweets',\n",
       " 'cargar uno Información',\n",
       " 'cargar uno que',\n",
       " 'cargar uno se',\n",
       " 'cargar uno va',\n",
       " 'cargar uno a',\n",
       " 'cargar uno de',\n",
       " 'cargar uno cada',\n",
       " 'cargar uno de',\n",
       " 'cargar uno los',\n",
       " 'cargar uno tweets',\n",
       " 'cargar de Información',\n",
       " 'cargar de que',\n",
       " 'cargar de se',\n",
       " 'cargar de va',\n",
       " 'cargar de a',\n",
       " 'cargar de de',\n",
       " 'cargar de cada',\n",
       " 'cargar de uno',\n",
       " 'cargar de los',\n",
       " 'cargar de tweets',\n",
       " 'cargar los Información',\n",
       " 'cargar los que',\n",
       " 'cargar los se',\n",
       " 'cargar los va',\n",
       " 'cargar los a',\n",
       " 'cargar los de',\n",
       " 'cargar los cada',\n",
       " 'cargar los uno',\n",
       " 'cargar los de',\n",
       " 'cargar los tweets',\n",
       " 'cargar tweets Información',\n",
       " 'cargar tweets que',\n",
       " 'cargar tweets se',\n",
       " 'cargar tweets va',\n",
       " 'cargar tweets a',\n",
       " 'cargar tweets de',\n",
       " 'cargar tweets cada',\n",
       " 'cargar tweets uno',\n",
       " 'cargar tweets de',\n",
       " 'cargar tweets los',\n",
       " 'de Información que',\n",
       " 'de Información se',\n",
       " 'de Información va',\n",
       " 'de Información a',\n",
       " 'de Información cargar',\n",
       " 'de Información cada',\n",
       " 'de Información uno',\n",
       " 'de Información de',\n",
       " 'de Información los',\n",
       " 'de Información tweets',\n",
       " 'de que Información',\n",
       " 'de que se',\n",
       " 'de que va',\n",
       " 'de que a',\n",
       " 'de que cargar',\n",
       " 'de que cada',\n",
       " 'de que uno',\n",
       " 'de que de',\n",
       " 'de que los',\n",
       " 'de que tweets',\n",
       " 'de se Información',\n",
       " 'de se que',\n",
       " 'de se va',\n",
       " 'de se a',\n",
       " 'de se cargar',\n",
       " 'de se cada',\n",
       " 'de se uno',\n",
       " 'de se de',\n",
       " 'de se los',\n",
       " 'de se tweets',\n",
       " 'de va Información',\n",
       " 'de va que',\n",
       " 'de va se',\n",
       " 'de va a',\n",
       " 'de va cargar',\n",
       " 'de va cada',\n",
       " 'de va uno',\n",
       " 'de va de',\n",
       " 'de va los',\n",
       " 'de va tweets',\n",
       " 'de a Información',\n",
       " 'de a que',\n",
       " 'de a se',\n",
       " 'de a va',\n",
       " 'de a cargar',\n",
       " 'de a cada',\n",
       " 'de a uno',\n",
       " 'de a de',\n",
       " 'de a los',\n",
       " 'de a tweets',\n",
       " 'de cargar Información',\n",
       " 'de cargar que',\n",
       " 'de cargar se',\n",
       " 'de cargar va',\n",
       " 'de cargar a',\n",
       " 'de cargar cada',\n",
       " 'de cargar uno',\n",
       " 'de cargar de',\n",
       " 'de cargar los',\n",
       " 'de cargar tweets',\n",
       " 'de cada Información',\n",
       " 'de cada que',\n",
       " 'de cada se',\n",
       " 'de cada va',\n",
       " 'de cada a',\n",
       " 'de cada cargar',\n",
       " 'de cada uno',\n",
       " 'de cada de',\n",
       " 'de cada los',\n",
       " 'de cada tweets',\n",
       " 'de uno Información',\n",
       " 'de uno que',\n",
       " 'de uno se',\n",
       " 'de uno va',\n",
       " 'de uno a',\n",
       " 'de uno cargar',\n",
       " 'de uno cada',\n",
       " 'de uno de',\n",
       " 'de uno los',\n",
       " 'de uno tweets',\n",
       " 'de de Información',\n",
       " 'de de que',\n",
       " 'de de se',\n",
       " 'de de va',\n",
       " 'de de a',\n",
       " 'de de cargar',\n",
       " 'de de cada',\n",
       " 'de de uno',\n",
       " 'de de los',\n",
       " 'de de tweets',\n",
       " 'de los Información',\n",
       " 'de los que',\n",
       " 'de los se',\n",
       " 'de los va',\n",
       " 'de los a',\n",
       " 'de los cargar',\n",
       " 'de los cada',\n",
       " 'de los uno',\n",
       " 'de los de',\n",
       " 'de los tweets',\n",
       " 'de tweets Información',\n",
       " 'de tweets que',\n",
       " 'de tweets se',\n",
       " 'de tweets va',\n",
       " 'de tweets a',\n",
       " 'de tweets cargar',\n",
       " 'de tweets cada',\n",
       " 'de tweets uno',\n",
       " 'de tweets de',\n",
       " 'de tweets los',\n",
       " 'cada Información que',\n",
       " 'cada Información se',\n",
       " 'cada Información va',\n",
       " 'cada Información a',\n",
       " 'cada Información cargar',\n",
       " 'cada Información de',\n",
       " 'cada Información uno',\n",
       " 'cada Información de',\n",
       " 'cada Información los',\n",
       " 'cada Información tweets',\n",
       " 'cada que Información',\n",
       " 'cada que se',\n",
       " 'cada que va',\n",
       " 'cada que a',\n",
       " 'cada que cargar',\n",
       " 'cada que de',\n",
       " 'cada que uno',\n",
       " 'cada que de',\n",
       " 'cada que los',\n",
       " 'cada que tweets',\n",
       " 'cada se Información',\n",
       " 'cada se que',\n",
       " 'cada se va',\n",
       " 'cada se a',\n",
       " 'cada se cargar',\n",
       " 'cada se de',\n",
       " 'cada se uno',\n",
       " 'cada se de',\n",
       " 'cada se los',\n",
       " 'cada se tweets',\n",
       " 'cada va Información',\n",
       " 'cada va que',\n",
       " 'cada va se',\n",
       " 'cada va a',\n",
       " 'cada va cargar',\n",
       " 'cada va de',\n",
       " 'cada va uno',\n",
       " 'cada va de',\n",
       " 'cada va los',\n",
       " 'cada va tweets',\n",
       " 'cada a Información',\n",
       " 'cada a que',\n",
       " 'cada a se',\n",
       " 'cada a va',\n",
       " 'cada a cargar',\n",
       " 'cada a de',\n",
       " 'cada a uno',\n",
       " 'cada a de',\n",
       " 'cada a los',\n",
       " 'cada a tweets',\n",
       " 'cada cargar Información',\n",
       " 'cada cargar que',\n",
       " 'cada cargar se',\n",
       " 'cada cargar va',\n",
       " 'cada cargar a',\n",
       " 'cada cargar de',\n",
       " 'cada cargar uno',\n",
       " 'cada cargar de',\n",
       " 'cada cargar los',\n",
       " 'cada cargar tweets',\n",
       " 'cada de Información',\n",
       " 'cada de que',\n",
       " 'cada de se',\n",
       " 'cada de va',\n",
       " 'cada de a',\n",
       " 'cada de cargar',\n",
       " 'cada de uno',\n",
       " 'cada de de',\n",
       " 'cada de los',\n",
       " 'cada de tweets',\n",
       " 'cada uno Información',\n",
       " 'cada uno que',\n",
       " 'cada uno se',\n",
       " 'cada uno va',\n",
       " 'cada uno a',\n",
       " 'cada uno cargar',\n",
       " 'cada uno de',\n",
       " 'cada uno de',\n",
       " 'cada uno los',\n",
       " 'cada uno tweets',\n",
       " 'cada de Información',\n",
       " 'cada de que',\n",
       " 'cada de se',\n",
       " 'cada de va',\n",
       " 'cada de a',\n",
       " 'cada de cargar',\n",
       " 'cada de de',\n",
       " 'cada de uno',\n",
       " 'cada de los',\n",
       " 'cada de tweets',\n",
       " 'cada los Información',\n",
       " 'cada los que',\n",
       " 'cada los se',\n",
       " 'cada los va',\n",
       " 'cada los a',\n",
       " 'cada los cargar',\n",
       " 'cada los de',\n",
       " 'cada los uno',\n",
       " 'cada los de',\n",
       " 'cada los tweets',\n",
       " 'cada tweets Información',\n",
       " 'cada tweets que',\n",
       " 'cada tweets se',\n",
       " 'cada tweets va',\n",
       " 'cada tweets a',\n",
       " 'cada tweets cargar',\n",
       " 'cada tweets de',\n",
       " 'cada tweets uno',\n",
       " 'cada tweets de',\n",
       " 'cada tweets los',\n",
       " 'uno Información que',\n",
       " 'uno Información se',\n",
       " 'uno Información va',\n",
       " 'uno Información a',\n",
       " 'uno Información cargar',\n",
       " 'uno Información de',\n",
       " 'uno Información cada',\n",
       " 'uno Información de',\n",
       " 'uno Información los',\n",
       " 'uno Información tweets',\n",
       " 'uno que Información',\n",
       " 'uno que se',\n",
       " 'uno que va',\n",
       " 'uno que a',\n",
       " 'uno que cargar',\n",
       " 'uno que de',\n",
       " 'uno que cada',\n",
       " 'uno que de',\n",
       " 'uno que los',\n",
       " 'uno que tweets',\n",
       " 'uno se Información',\n",
       " 'uno se que',\n",
       " 'uno se va',\n",
       " 'uno se a',\n",
       " 'uno se cargar',\n",
       " 'uno se de',\n",
       " 'uno se cada',\n",
       " 'uno se de',\n",
       " 'uno se los',\n",
       " 'uno se tweets',\n",
       " 'uno va Información',\n",
       " 'uno va que',\n",
       " 'uno va se',\n",
       " 'uno va a',\n",
       " 'uno va cargar',\n",
       " 'uno va de',\n",
       " 'uno va cada',\n",
       " 'uno va de',\n",
       " 'uno va los',\n",
       " 'uno va tweets',\n",
       " 'uno a Información',\n",
       " 'uno a que',\n",
       " 'uno a se',\n",
       " 'uno a va',\n",
       " 'uno a cargar',\n",
       " 'uno a de',\n",
       " 'uno a cada',\n",
       " 'uno a de',\n",
       " 'uno a los',\n",
       " 'uno a tweets',\n",
       " 'uno cargar Información',\n",
       " 'uno cargar que',\n",
       " 'uno cargar se',\n",
       " 'uno cargar va',\n",
       " 'uno cargar a',\n",
       " 'uno cargar de',\n",
       " 'uno cargar cada',\n",
       " 'uno cargar de',\n",
       " 'uno cargar los',\n",
       " 'uno cargar tweets',\n",
       " 'uno de Información',\n",
       " 'uno de que',\n",
       " 'uno de se',\n",
       " 'uno de va',\n",
       " 'uno de a',\n",
       " 'uno de cargar',\n",
       " 'uno de cada',\n",
       " 'uno de de',\n",
       " 'uno de los',\n",
       " 'uno de tweets',\n",
       " 'uno cada Información',\n",
       " 'uno cada que',\n",
       " 'uno cada se',\n",
       " 'uno cada va',\n",
       " 'uno cada a',\n",
       " 'uno cada cargar',\n",
       " 'uno cada de',\n",
       " 'uno cada de',\n",
       " 'uno cada los',\n",
       " 'uno cada tweets',\n",
       " 'uno de Información',\n",
       " 'uno de que',\n",
       " 'uno de se',\n",
       " 'uno de va',\n",
       " 'uno de a',\n",
       " 'uno de cargar',\n",
       " 'uno de de',\n",
       " 'uno de cada',\n",
       " 'uno de los',\n",
       " 'uno de tweets',\n",
       " 'uno los Información',\n",
       " 'uno los que',\n",
       " 'uno los se',\n",
       " 'uno los va',\n",
       " 'uno los a',\n",
       " 'uno los cargar',\n",
       " 'uno los de',\n",
       " 'uno los cada',\n",
       " 'uno los de',\n",
       " 'uno los tweets',\n",
       " 'uno tweets Información',\n",
       " 'uno tweets que',\n",
       " 'uno tweets se',\n",
       " 'uno tweets va',\n",
       " 'uno tweets a',\n",
       " 'uno tweets cargar',\n",
       " 'uno tweets de',\n",
       " 'uno tweets cada',\n",
       " 'uno tweets de',\n",
       " 'uno tweets los',\n",
       " 'de Información que',\n",
       " 'de Información se',\n",
       " 'de Información va',\n",
       " 'de Información a',\n",
       " 'de Información cargar',\n",
       " 'de Información de',\n",
       " 'de Información cada',\n",
       " 'de Información uno',\n",
       " 'de Información los',\n",
       " 'de Información tweets',\n",
       " ...]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from itertools import permutations\n",
    "\n",
    "[\" \".join(combo) for combo in permutations(text.split(), 3)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "479001600"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = text\n",
    "\n",
    "len(set(s.split()))\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "np.arange(1, 12 + 1).prod() # factorial(12) = arange(1, 13).prod()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prooceso General"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalizacion de Textos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dividir por espacios en blanco"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Información',\n",
       " 'que',\n",
       " 'se',\n",
       " 'va',\n",
       " 'a',\n",
       " 'cargar',\n",
       " 'de',\n",
       " 'cada',\n",
       " 'uno',\n",
       " 'de',\n",
       " 'los',\n",
       " 'tweets']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[0:100].split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remover Caracteres especiales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "print(string.punctuation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{33: None,\n",
       " 34: None,\n",
       " 35: None,\n",
       " 36: None,\n",
       " 37: None,\n",
       " 38: None,\n",
       " 39: None,\n",
       " 40: None,\n",
       " 41: None,\n",
       " 42: None,\n",
       " 43: None,\n",
       " 44: None,\n",
       " 45: None,\n",
       " 46: None,\n",
       " 47: None,\n",
       " 58: None,\n",
       " 59: None,\n",
       " 60: None,\n",
       " 61: None,\n",
       " 62: None,\n",
       " 63: None,\n",
       " 64: None,\n",
       " 91: None,\n",
       " 92: None,\n",
       " 93: None,\n",
       " 94: None,\n",
       " 95: None,\n",
       " 96: None,\n",
       " 123: None,\n",
       " 124: None,\n",
       " 125: None,\n",
       " 126: None}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tabla = str.maketrans('', '', string.punctuation)\n",
    "tabla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'text' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-8e03ba624b09>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdepurado\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranslate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtabla\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdepurado\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'text' is not defined"
     ]
    }
   ],
   "source": [
    "depurado = [w.translate(tabla) for w in text.split()]\n",
    "print(depurado[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pasar a minuscula"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = [word.lower() for word in text.split()]\n",
    "print(words[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Tokens:** Una base de datos de texto (o *corpus*) es una agrupación de bytes. El texto en su forma más pura, es una colección de bytes (o caracteres). La mayoria de veces es útil agrupar estos caracteres en unidades continuas llamadas tokens. En español, al igual que en la mayoria de los idiomas occidentales, un token corresponde a palabras y sequencias numericas separadas por espacios en blanco o signos de puntuación.  El proceso de reducir un texto a tokens se conoce como **tokenización**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Información, que, se, va, a, cargar, de, cada, uno, de]\n"
     ]
    }
   ],
   "source": [
    "parsed_text=nlp(text)\n",
    "tokens=[]\n",
    "for token in parsed_text:\n",
    "    tokens.append(token)\n",
    "    \n",
    "print(tokens[0:10])    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### n-gramas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Unigramas, bigramas, trigramas,...,N-gramas**: Corresponden a una colección de tokens secuenciales que ocurren en un texto. En este caso, un bigrama corresponde a dos tokens que occuren en forma consecutiva. Un trigrama es una colección de tres tokens, etc. \n",
    "\n",
    "Es bastante sencillo crear N-gramas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def n_grams(texto,n):\n",
    "  '''\n",
    "  toma tokes o texto y retorna una lista de n-gramas\n",
    "  '''\n",
    "  return [texto[i:i+n] for i in range(len(text)-n+1)]\n",
    "\n",
    "bigrama = n_grams(parsed_text,2)\n",
    "\n",
    "print(bigrama[0:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Colocación (collocations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " import nltk\n",
    " from nltk.collocations import *\n",
    " bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    " trigram_measures = nltk.collocations.TrigramAssocMeasures()\n",
    " finder = BigramCollocationFinder.from_words(tokens)\n",
    " finder.nbest(bigram_measures.pmi, 10)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " finder = TrigramCollocationFinder.from_words(tokens)\n",
    " finder.nbest(trigram_measures.pmi, 10) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extración de Oraciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 0\n",
    "sentencias = []\n",
    "for sentence in parsed_text.sents:\n",
    "    sentencias.append(sentence)\n",
    "    print('Numero de sentencia', idx, ':', sentence)\n",
    "    idx += 1\n",
    "    \n",
    "    if idx == 10:  break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Lemmas**: Lemas corresponde a la raíz de una palabra. Considere por ejemplo la palabra *correr*. Esta puede tener distintas formas como *corriendo, corrí, correré,* entre otras. En este caso, resulta útil reducir la palabra a su raíz o lemma. En este caso, este proceso se conoce como *lematization*. Obtener los lemas es bastante sencillo una vez se ha procesado el texto con la funcion nlp de Spacy, y se logra a través de la función .lemma_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in tokens[0:20]:\n",
    "    print('{} --> {}'.format(token, token.lemma_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stop word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stop words\n",
    "#Importar stop wrd en espanol.\n",
    "import spacy\n",
    "spacy_stopwords = spacy.lang.es.stop_words.STOP_WORDS\n",
    "\n",
    "#Mostrarelnumero total de stop words:\n",
    "print('Numero de stop words: %d' % len(spacy_stopwords))\n",
    "\n",
    "#Mostrar un top de stop words:\n",
    "print('El top de las stop word: %s' % list(spacy_stopwords)[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(parsed_text[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.lang.es.stop_words import STOP_WORDS\n",
    "\n",
    "#Implementation of stop words:\n",
    "text_filtrado=[]\n",
    "\n",
    "# filtering stop words\n",
    "for word in parsed_text:\n",
    "    if word.is_stop==False:\n",
    "        text_filtrado.append(word)\n",
    "print(\"Texto_Filtrado:\",text_filtrado[0:60])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_stop=[]\n",
    "\n",
    "# filtering stop words\n",
    "for word in parsed_text:\n",
    "    if word.is_stop==True:\n",
    "        text_stop.append(word)\n",
    "print(\"Texto_Filtrado:\",text_stop[0:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inclusión de stop word propias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mis_stopwords=['a','y', 'para']\n",
    "\n",
    "for word in mis_stopwords:\n",
    "    nlp.vocab[word].is_stop=True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Categorización de oraciones y documentos**: Una de las primeras aplicaciones de NLP corresponde a la categorización de grandes porciones de texto de un documento. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### POS tagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uno de las categorizaciones más comunes corresponde a *Part of Speech tagging*, que es simplemente conocer si una determinada palabra corresponde a un verbo, adjetivo, sujeto u otra estructura de una oración:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Información - NOUN\n",
      "que - PRON\n",
      "se - PRON\n",
      "va - AUX\n",
      "a - ADP\n",
      "cargar - VERB\n",
      "de - ADP\n",
      "cada - DET\n",
      "uno - PRON\n",
      "de - ADP\n",
      "los - DET\n",
      "tweets - NOUN\n"
     ]
    }
   ],
   "source": [
    "for token in tokens[0:20]:\n",
    "    print('{} - {}'.format(token, token.pos_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Labels:* Spacy nos deja identificar si una palabra corresponde a un lugar, persona u organización en particular. Esto se logra usando la función .ents() e imprimiento los labels. Claro, como todo modelo, esto no es perfecto, pero es una ayuda!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tabulate import tabulate \n",
    "\n",
    "table = [[\"Text\", \"Lemma\", \"POS\", \"Tag\", \"Dep\", \"Alpha\", \"Stop\"]]\n",
    "for token in tokens[:20]:\n",
    "    table.append([\n",
    "        token.text, token.lemma_, token.pos_, token.tag_, \n",
    "        token.dep_, token.is_alpha, token.is_stop\n",
    "    ])\n",
    "print(tabulate(table, tablefmt=\"simple\", headers=\"firstrow\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analísis de Dependencias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![parsing2](img/parsing2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy import displacy\n",
    "\n",
    "\n",
    "doc = nlp(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Llevar los resultados a un Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Para termonis de flexibilidad llevesmo algunos datos a un dataframe \n",
    "# de pandas que nos permita interactuar con otros metodos\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame()\n",
    "\n",
    "for i, token in enumerate(parsed_text):\n",
    "    df.loc[i, 'text'] = token.text\n",
    "    df.loc[i, 'lemma'] = token.lemma_\n",
    "    df.loc[i, 'pos'] = token.pos_\n",
    "    df.loc[i, 'dep'] = token.dep_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expresiones Regulares Usando taggers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tag.sequential import RegexpTagger\n",
    "regexp_tagger = RegexpTagger(\n",
    "[( r'^-?[0-9]+(.[0-9]+)?$', 'CD'), # úmeros cardinales\n",
    "( r'(The|the|A|a|An|an)$', 'AT'), # articulos\n",
    "( r'.*able$', 'JJ'), # adjetivos\n",
    "( r'.*ness$', 'NN'), # sustantivosformados por adjetivos\n",
    "( r'.*ly$', 'RB'), # advervios\n",
    "( r'.*s$', 'NNS'), # sustantivos plurales\n",
    "( r'.*ing$', 'VBG'), # gerundios\n",
    "(r'.*ed$', 'VBD'), # vervos en pasado\n",
    "(r'.*', 'NN') # sustantivos\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "regexp_tagger = RegexpTagger(\n",
    "[\n",
    "(r'.*ed$', 'VBD'), # verbos en pasado\n",
    "\n",
    "])\n",
    "\n",
    "regexp_tagger.tag(nltk.word_tokenize(text[0:150]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 0\n",
    "sentencias = []\n",
    "for sentence in parsed_text.sents:\n",
    "    sentencias.append(sentence)\n",
    "    print('Numero de sentencia', idx, ':', sentence)\n",
    "    idx += 1\n",
    "    \n",
    "    if idx == 10:  break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    from collections import Counter\n",
    "\n",
    "    histogram_with_some_filtering = Counter()\n",
    "\n",
    "    for token in parsed_text:\n",
    "        lemma = token.lemma_.lower()\n",
    "        if not (nlp.vocab[lemma].is_stop or token.pos_ == 'PUNCT' or token.pos_ == 'SPACE'):\n",
    "            histogram_with_some_filtering[lemma] += 1\n",
    "\n",
    "    from operator import itemgetter\n",
    "    sorted_lemma_count_pairs = sorted(histogram_with_some_filtering.items(),\n",
    "                                      reverse=True,\n",
    "                                      key=itemgetter(1))\n",
    "    for lemma, count in sorted_lemma_count_pairs[0:15]:\n",
    "        print(lemma, \":\", count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*a* y *y* son de hecho stop words. se incluyen estas palabras al listado de stopwords de spacy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mis_stopwords=['a','y', 'para']\n",
    "\n",
    "for word in mis_stopwords:\n",
    "    nlp.vocab[word].is_stop=True\n",
    "    \n",
    "histogram_with_some_filtering = Counter()\n",
    "\n",
    "for token in parsed_text:\n",
    "    lemma = token.lemma_.lower()\n",
    "    if not (nlp.vocab[token.orth_.lower()].is_stop or token.pos_ == 'PUNCT' or token.pos_ == 'SPACE'):\n",
    "        histogram_with_some_filtering[lemma] += 1\n",
    "    \n",
    "from operator import itemgetter\n",
    "sorted_lemma_count_pairs = sorted(histogram_with_some_filtering.items(),\n",
    "                                  reverse=True,\n",
    "                                  key=itemgetter(1))\n",
    "for lemma, count in sorted_lemma_count_pairs[0:15]:\n",
    "    print(lemma, \":\", count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('seaborn')  # prettier plots (for example, use 'ggplot' instead of 'seaborn' for plots like in R)\n",
    "%config InlineBackend.figure_format = 'retina'  # if you use a Mac with Retina display\n",
    "\n",
    "num_top_lemmas_to_plot = 20\n",
    "top_lemmas = [lemma for lemma, count in sorted_lemma_count_pairs[:num_top_lemmas_to_plot]]\n",
    "top_counts = [count for lemma, count in sorted_lemma_count_pairs[:num_top_lemmas_to_plot]]\n",
    "plt.bar(range(num_top_lemmas_to_plot), top_counts)\n",
    "plt.xticks(range(num_top_lemmas_to_plot), top_lemmas, rotation=45)\n",
    "plt.xlabel('Lemma')\n",
    "plt.ylabel('Raw count')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformacion de texto (text embedding):\n",
    "\n",
    "La transformacion de texto a números es una condición necesaria para que el texto, o cualquier archivo multimedia, sea procesado por el computador. Para tal fin, es necesario preservar el significado semantico del texto durante tal transformación. A esto se le conoce como embedding. Por ejemplo, *\"ancla\"* y *\"bote\"* tienen un embedding cercano, mientras que *\"ancla\"* y *\"perro\"* no...\n",
    "\n",
    "Algunos embeddings populares son:\n",
    "\n",
    "    - Word2Vec: Desarrollado por Google en 2013, se ha convertido en el text embedding por default de la industria. Es una red neuronal que es capaz de generar analogias del tipo: rey es a hombre como reina a mujer. \n",
    "    \n",
    "    -GlobVe: Mejorado a partir del Word2Vec para el aprendizaje de vocabulario, siendo mas eficiente en el aprendizaje de nuevo vocabulario."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PMI y Análisis de Coocurrencias:\n",
    "\n",
    "PMI (Pointwise Mutual Index) es una medida de asociación entre dos palabras en particular *x* y *y*. Es decir, el PMI de dos palabras cuantifica la discrepancia entre la coincidencia de dos Variables Aleatorias discretas dada la distribución conjunta de probabilidad y las distribuciones individuales, asumiendo independencia. Formalmente:\n",
    "\n",
    "\\begin{equation}\n",
    "\\operatorname {pmi} (x;y)\\equiv \\log {\\frac {p(x,y)}{p(x)p(y)}}=\\log {\\frac {p(x|y)}{p(x)}}=\\log {\\frac {p(y|x)}{p(y)}}.\n",
    "\\end{equation}\n",
    "\n",
    "Este tipo de análisis son útiles para poder entender si dos palabras sueles estar asociadas y ocurren con frecuencia. Para ilustrarlo, suponga que existe la siguiente tabla de coocurrencia para las siguientes entidades:\n",
    "\n",
    "| <i></i>         | Apple | Facebook | Tesla |\n",
    "| --------------- |:-----:|:--------:|:-----:|\n",
    "| Elon Musk       | 10    | 15       | 300   |\n",
    "| Mark Zuckerberg | 500   | 10000    | 500   |\n",
    "| Tim Cook        | 200   | 30       | 10    |\n",
    "\n",
    "En este caso, Elon Musk y Apple co-ocurren en 10 articulos. Facebook y Elon Musk ocurren 15 veces... y así para cada caso.\n",
    "\n",
    "Para la práctica, usaremos un corpus de Reuters en inglés, por lo que debemos descargar el diccionario de SpaCy en dicho idioma, y realizar su cargue. A continuación, realizaremos algunos análisis sobre las entidades que aparecen en los documentos, y realizaremos un análisis de coocurrencia para conocer que suele estar asociado con nuestro país"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "####Ejemplo tomado de Curso de texto dictado en Bancolobia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from nltk.corpus import reuters\n",
    "import re\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load('es_core_news_md')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nltk' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-6e230a00a763>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'nltk' is not defined"
     ]
    }
   ],
   "source": [
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reuters_archivos= reuters.fileids()\n",
    "reuters_nlp = [nlp(re.sub('\\s+',' ', reuters.raw(i)).strip()) for i in reuters_archivos]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contador_palabra=Counter()\n",
    "contador_articulo=Counter()\n",
    "for i in reuters_nlp:\n",
    "    for entity in i.ents:\n",
    "        if entity.label_=='NORP':\n",
    "            entity_name=entity.text.strip()\n",
    "            contador_palabra[entity_name]+=1\n",
    "    for item in contador_palabra.keys():\n",
    "        if contador_palabra[item]>0:\n",
    "            contador_articulo[item]+=1\n",
    "            contador_palabra[item]=0\n",
    "\n",
    "contador_articulo.most_common(10)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contador_palabra=Counter()\n",
    "contador_articulo=Counter()\n",
    "for i in reuters_nlp:\n",
    "    for entity in i.ents:\n",
    "        if entity.label_=='GPE':\n",
    "            entity_name=entity.text.strip()\n",
    "            contador_palabra[entity_name]+=1\n",
    "    for item in contador_palabra.keys():\n",
    "        if contador_palabra[item]>0:\n",
    "            contador_articulo[item]+=1\n",
    "            contador_palabra[item]=0\n",
    "\n",
    "contador_articulo.most_common(10)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora procedemos a encontrar el PMI para el pais \"GPE\" de Colombia y encontrar las personas con mayor PMI asociado a este país. En este caso, $P('colombia') = \\frac{número \\ \\ de \\ \\ documentos \\ \\ que \\ \\ contienen \\ \\ 'colombia'}{número \\ \\ de \\ \\ documentos}$.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "total_art=0\n",
    "num_col=0\n",
    "is_col=False\n",
    "articulos_palabras_joint=Counter()\n",
    "articulos_palabras_joint_final=Counter()\n",
    "prob_joint=Counter()\n",
    "for i in reuters_nlp:\n",
    "    total_art+=1\n",
    "    for entity in i.ents:\n",
    "        if entity.text.lower().strip()=='colombia':\n",
    "            is_col=True\n",
    "    if is_col==True:\n",
    "        num_col+=1\n",
    "        is_col=False\n",
    "                \n",
    "prob_col=num_col/total_art\n",
    "\n",
    "####### Prob for each word of GPE:\n",
    "num_persona_palabra=Counter()\n",
    "num_persona_articulo=Counter()\n",
    "for i in reuters_nlp:\n",
    "    for entity in i.ents:\n",
    "        if entity.label_=='PERSON':\n",
    "            entity_name=entity.text.lower().strip()\n",
    "            num_persona_palabra[entity_name]+=1\n",
    "    for item in num_persona_palabra.keys():\n",
    "        if num_persona_palabra[item]>0:\n",
    "            num_persona_articulo[item]+=1\n",
    "            num_persona_palabra[item]=0\n",
    "\n",
    "probabilidad_persona=Counter()\n",
    "for k in num_persona_articulo.keys():\n",
    "    probabilidad_persona[k]=num_persona_articulo[k]/total_art\n",
    "\n",
    "#### Joint Prob\n",
    "num_joint_texto=Counter()\n",
    "num_final=Counter()\n",
    "for i in reuters_nlp:\n",
    "    is_col=False\n",
    "    for entity in i.ents:\n",
    "        if entity.text.lower().strip()=='colombia':\n",
    "            is_col=True\n",
    "    if is_col==True:\n",
    "        for item in i.ents:\n",
    "            if item.label_=='PERSON':\n",
    "                entity_name=item.text.lower().strip()\n",
    "                num_joint_texto[entity_name]+=1\n",
    "        for alpha in num_joint_texto.keys():\n",
    "            if num_joint_texto[alpha]>0:\n",
    "                num_final[alpha]+=1\n",
    "                num_joint_texto[alpha]=0\n",
    "        \n",
    "\n",
    "probabilidad_joint=Counter()\n",
    "for i in num_final.keys():\n",
    "    probabilidad_joint[i]=num_final[i]/num_col\n",
    "####Computing the PMI:\n",
    "\n",
    "PMI_final=Counter()\n",
    "for i in probabilidad_joint.keys():\n",
    "    PMI_final[i]=np.log(probabilidad_joint[i]/(probabilidad_persona[i]*prob_col))\n",
    "\n",
    "PMI_final.most_common(50)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reducción de dimensionalidad en textos: PCA, Isomaps y t-SNE\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import codecs\n",
    "\n",
    "file_list_business = glob.glob(os.path.join(os.getcwd(), \"bbc/business\", \"*.txt\"))\n",
    "\n",
    "corpus_business = []\n",
    "\n",
    "for file_path in file_list_business:\n",
    "    with open(file_path) as f_input:\n",
    "        corpus_business.append(f_input.read())\n",
    "\n",
    "corpus_business=[x.encode('utf-8') for x in corpus_business]\n",
    "\n",
    "file_list_entertainment = glob.glob(os.path.join(os.getcwd(), \"bbc/entertainment\", \"*.txt\"))\n",
    "\n",
    "corpus_entertainment = []\n",
    "\n",
    "for file_path in file_list_entertainment:\n",
    "    with open(file_path) as f_input:\n",
    "        corpus_entertainment.append(f_input.read())\n",
    "corpus_entertainment=[x.encode('utf-8') for x in corpus_entertainment]\n",
    "\n",
    "\n",
    "file_list_politics = glob.glob(os.path.join(os.getcwd(), \"bbc/politics\", \"*.txt\"))\n",
    "\n",
    "corpus_politics = []\n",
    "\n",
    "for file_path in file_list_politics:\n",
    "    with open(file_path) as f_input:\n",
    "        corpus_politics.append(f_input.read())\n",
    "corpus_politics=[x.encode('utf-8') for x in corpus_politics]\n",
    "\n",
    "\n",
    "file_list_sport = glob.glob(os.path.join(os.getcwd(), \"bbc/sport\", \"*.txt\"))\n",
    "\n",
    "corpus_sport = []\n",
    "\n",
    "for file_path in file_list_sport:\n",
    "    with open(file_path) as f_input:\n",
    "        corpus_sport.append(f_input.read())\n",
    "corpus_sport=[x.encode('utf-8') for x in corpus_sport]\n",
    "\n",
    "file_list_tech = glob.glob(os.path.join(os.getcwd(), \"bbc/tech\", \"*.txt\"))\n",
    "corpus_tech = []\n",
    "\n",
    "for file_path in file_list_tech:\n",
    "    with open(file_path) as f_input:\n",
    "        corpus_tech.append(f_input.read())\n",
    "corpus_tech=[x.encode('utf-8') for x in corpus_tech]\n",
    "\n",
    "corpus=[]\n",
    "\n",
    "for i in corpus_business:\n",
    "    corpus.append(i)\n",
    "    \n",
    "for i in corpus_entertainment:\n",
    "    corpus.append(i)\n",
    "    \n",
    "for i in corpus_sport:\n",
    "    corpus.append(i)\n",
    "\n",
    "for i in corpus_tech:\n",
    "    corpus.append(i)\n",
    "\n",
    "for i in corpus_politics:\n",
    "    corpus.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer=TfidfVectorizer(min_df=50, stop_words=\"english\", max_df=0.8)\n",
    "X=vectorizer.fit_transform(corpus)\n",
    "X=X.todense()\n",
    "vocabulary=vectorizer.vocabulary_\n",
    "print(\"En número de palabras únicas es: \", len(vocabulary))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Análisis de Componentes Principales:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "reduccion_pca = PCA(n_components=2)  #Cargamos el modelo, especificando el número de dimensiones.\n",
    "reduccion_texto = reduccion_pca.fit_transform(X) #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.scatter(reduccion_texto[:,0], reduccion_texto[:,1], alpha=0.5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
